% Introduce H_k and the problem
First, let's note $\abs{x_{\langle i\rangle}}$ the $i\textsuperscript{th}$ biggest (in absolute value) component of the vector $x$. The best approximation (w.r.t the $l_2$ norm) in $C_k$ for a determined $k\in\NN^*$ of a signal $x\in\RR^n$ is noted $H_k(x)$ and we define its components with 
\[\big(H_k(x)\big)_j = \begin{cases} x_j,\text{ if }x_j \geq \abs{x_{\langle k\rangle}} \\
0, \text{ else }\end{cases} \forall j =1,\dots, n\enspace. \]
Now, we can try to find an expression for $\mathcal{S}_k(x)$.

% Proposition conjugate of s
\begin{proposition}\label{prop:skstar}
Let $y\in\RR^n$ and $k\in\NN^*$, then $s_k^*(y) = \frac{1}{2}\|H_k(y)\|^2_2$.
\end{proposition}

% its proof
\begin{proof}
Using \eqref{eq:conjugate}, we know that for $y\in\RR^n$,
\[
\begin{aligned}
s_k^*(y) & = \max_{x\in\RR^n}\left[\langle x,y\rangle - s_k(x)\right] = \max_{x\in\RR^n}\left[\langle x,y\rangle - \frac{\|x\|_2^2}{2}\right]
          = \max_{x\in\RR^n}\left[-\left(\frac{\|x\|^2_2}{2} - \langle x, y\rangle\right) \right]\\
          &= \max_{x\in\RR^n}\left[-\frac{\|x-y\|^2_2}{2} + \frac{\|y\|^2_2}{2} \right] \enspace.
\end{aligned}
\]
Besides, $H_k(x)\in\arg\min_{y\in C_k}\|x-y\|_2$ and $\|y\|_2^2$ is independent of $x$, so 
\[s_k^*(y) -\frac{1}{2}\|H_k(y) - y\|^2_2 + \frac{1}{2}\|y\|_2^2\enspace.\]
And $-\|H_k(y) -y\|^2_2 = -\|H_k(y)\|_2^2 - \|y\|^2_2 + 2 \langle H_k(y),y\rangle$. But because $(H_k(y))_j=0$ for the $n-k$ smallest absolute values of $y$, $\langle H_k(y),y\rangle = \|H_k(y)\|_2^2$. Thus, we obtain by plugging-in this value:
\[s_k^*(y) = -\frac{1}{2}\|H_k(y)\|_2^2\enspace.\]
\end{proof}

% Begin the biconjugate expression
To find the biconjugate of $s_k(y)$, we now only have to find the conjugate of $-\frac{1}{2}\|H_k(y)\|^2_2$. Let's introduce the space $D_k=\{u\in\RR^n\,|\,\sum_{i=1}^n u_i \leq k,\  0\leq u_i\leq 1\ \forall i\}$. Then 
\begin{equation}\label{eq:minmax}
\mathcal{S}_k(x) = \max_{y\in\RR^n}\min_{u\in D_k}\left\{\langle x, y\rangle - \frac{1}{2}\sum_{i=1}^n u_iy_i^2\right\}\enspace.
\end{equation}
Thanks to Sion's $minimax$ theorem \cite{10.2307/30037472} thanks to the convexity and concavity, we can inverse the $\min$ and $\max$ we can switch the two operators and get an expression of $\mathcal{S}_k(x)$.

% theorem with the linear over quadratic formula
\begin{proposition}\label{prop:Sfromphi} With the same notations we have 
\begin{equation}\mathcal{S}_k(x) = \frac{1}{2}\min_{u\in D_k} \sum_{i=1}^n \phi(x_i, u_i)\text{, where }
\phi(x,u)=\begin{cases}\frac{x^2}{u},&\ \text{if }u>0\\ 0&\ \text{if }u=x=0\\ \infty &\text{else} \end{cases}\enspace.\end{equation}
\end{proposition}
\begin{proof}
We only have to consider the inner part of \eqref{eq:minmax}. Let $i_0\in\{1,\dots,n\}$ an index. We want to find the value at the optimum, we can calculate the first order conditions:
\[ \frac{\partial}{\partial y_{i_0}} \sum_{i=1}^n x_iy_i -\frac{1}{2}\sum_{i=1}^n u_iy_i^2 = x_{i_0} - \frac{1}{2}2u_{i_0}y_{i_0}\enspace.\]
At the optimum, equalizing to $0$, if $u_{i_0}>0$ we indeed get $\hat y_{i_0} = \frac{x_{i_0}}{u_{i_0}}$ and the other values are trivial in the other cases. Plugging-in the obtained value, we finally get the result for $u_i>0$ (the other cases are direct and the second derivative is indeed negative):
\[\max_{y\in\RR^n}\left\{x'y - \frac{1}{2}\sum_{i=1}^nu_iy_i^2\right\} = \sum_{i=1}^nx_i \frac{x_i}{u_i} - \frac{1}{2}\sum_{i=1}^n u_i \frac{x_i^2}{u_i} = \frac{1}{2}\sum_{i=1}^n \frac{x_i^2}{u_i}\enspace.\]
\end{proof}

% expression for S_k(x)
From there, it can be proved \cite{beck}, that if $\|x\|_0\leq k$ then $\mathcal{S}_k(x)=\|x\|_2^2 /2$. Otherwise, the expression depends on the root of a function defined as the sum of linear monotonous piecewise functions with a single breakpoint:
\[g_x(\eta)=\sum_{i=1}^n \min\{\abs{x_i}\eta,1\} - k,\ \eta\geq 0\enspace.\]
The general expression of $\mathcal{S}_k(x)$ is:
\begin{equation}
    S_k(x)=\frac{1}{2}\sum_{i=1}^{N_x} x_{\langle i \rangle}^2 + \frac{1}{2(k- N_x)}\left(\sum_{i=N_x+1}^n\abs{x_{\langle i\rangle}}^2\right)^2\enspace,
\end{equation}
where $N_x=\arg\max_{i=1,\dots,n}\left\{\abs{x_{\langle i\rangle}}\geq\tilde\eta ^{-1}\right\}$, with $\tilde \eta$ the root of $g_x$.